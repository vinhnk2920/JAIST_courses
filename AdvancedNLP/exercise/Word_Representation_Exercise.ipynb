{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EI1prha7yUf"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries if not already installed\n",
        "!pip install gensim matplotlib scikit-learn\n",
        "\n",
        "import gensim.downloader as api\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01lQ2GGXNOlX"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained word embeddings (GloVe)\n",
        "model = api.load('glove-wiki-gigaword-100')  # 100-dimensional embeddings\n",
        "#model = api.load('word2vec-google-news-300')  # 300-dimensional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9Md4UrCLjX3"
      },
      "outputs": [],
      "source": [
        "# Select a subset of words for visualization\n",
        "words = [\n",
        "    'king', 'queen', 'man', 'woman', 'prince', 'princess',\n",
        "    'apple', 'orange', 'banana', 'grape', 'fruit', 'vegetable',\n",
        "    'cat', 'dog', 'pet', 'animal', 'car', 'truck', 'vehicle',\n",
        "    'city', 'town', 'village', 'country', 'continent', 'planet'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uxCspy9QO4v"
      },
      "outputs": [],
      "source": [
        "#Example of Word Embedding\n",
        "model['man']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7gm-fMRNnXz"
      },
      "outputs": [],
      "source": [
        "# Get the word vectors for the selected words\n",
        "word_vectors = [model[word] for word in words]\n",
        "\n",
        "# Apply PCA to reduce dimensions to 2D\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(word_vectors)\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(12, 9))\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "\n",
        "# Annotate the points with the word labels\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(result[i, 0]+0.1, result[i, 1]+0.1))\n",
        "\n",
        "plt.title('PCA Visualization of Word Embeddings')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U0Wlpii8qQu"
      },
      "outputs": [],
      "source": [
        "# Get the word vectors for the selected words\n",
        "word_vectors = [model[word] for word in words]\n",
        "# Convert word_vectors to a NumPy array\n",
        "word_vectors = np.array(word_vectors)\n",
        "\n",
        "# Apply t-SNE to reduce dimensions to 2D\n",
        "tsne = TSNE(n_components=2, random_state=0, perplexity=5)\n",
        "result = tsne.fit_transform(word_vectors)\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(12, 9))\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "\n",
        "# Annotate the points with the word labels\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(\n",
        "        word,\n",
        "        xy=(result[i, 0] + 1, result[i, 1] + 1),\n",
        "        fontsize=12\n",
        "    )\n",
        "\n",
        "plt.title('t-SNE Visualization of Word Embeddings')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovx49pZBORNX"
      },
      "outputs": [],
      "source": [
        "#Finding the top n words that are similar to a target word is simple.\n",
        "#The result is the list of n words with the score.\n",
        "model.most_similar(positive=['mit'], topn = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPjL2BUBPxth"
      },
      "outputs": [],
      "source": [
        "# word analogy example\n",
        "# king is to man as what is to woman?\n",
        "king = model['king']\n",
        "man = model['man']\n",
        "woman = model['woman']\n",
        "\n",
        "# resulting vector\n",
        "result = king - man + woman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soP7_4G-RFaZ"
      },
      "outputs": [],
      "source": [
        "# function to compute cosine similarity\n",
        "def cosine_similarity(v1, v2):\n",
        "############# YOUR CODE HERE ################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9gIFVRjSMGJ"
      },
      "outputs": [],
      "source": [
        "# Get all words in the model's vocabulary\n",
        "allwords = [w for w in model.index_to_key if w != 'king' and w != 'man' and w != 'woman']\n",
        "\n",
        "# Calculate cosine similarity between the result vector and each word in the vocabulary\n",
        "############# YOUR CODE HERE ################\n",
        "\n",
        "# Ranking (Sort) the result based on the similarity score\n",
        "############# YOUR CODE HERE ################\n",
        "\n",
        "# Print the top 5 most similar words\n",
        "############# YOUR CODE HERE ################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
