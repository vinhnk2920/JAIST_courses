{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjNIhcqaE7Kn"
      },
      "outputs": [],
      "source": [
        "# Install the sklearn-crfsuite package, which provides tools for Conditional Random Fields (CRFs)\n",
        "# This package is used for sequence modeling, often applied to tasks like Named Entity Recognition (NER)\n",
        "!pip install sklearn-crfsuite\n",
        "\n",
        "# Import necessary libraries\n",
        "import nltk                      # Natural Language Toolkit for NLP tasks\n",
        "import pandas as pd              # Data manipulation and analysis\n",
        "import sklearn_crfsuite          # CRF suite for sequence modeling\n",
        "from sklearn.model_selection import train_test_split  # Function to split data into train and test sets\n",
        "import numpy as np               # Numerical operations on arrays\n",
        "from sklearn_crfsuite import metrics as crf_metrics  # Metrics for evaluating CRF models\n",
        "\n",
        "# Download NLTK resources:\n",
        "# 'punkt' is required for tokenization of text into sentences or words.\n",
        "# 'averaged_perceptron_tagger' is used for part-of-speech (POS) tagging.\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68rnIZa4FdBP"
      },
      "outputs": [],
      "source": [
        "# Load the dataset into a pandas DataFrame from a URL pointing to a compressed .csv file.\n",
        "# The file is in gzip format, so `compression='gzip'` is used to handle it directly.\n",
        "# The 'ISO-8859-1' encoding is specified to correctly interpret the character set used in the file.\n",
        "df = pd.read_csv('https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2008%20-%20Project%206%20-%20Build%20your%20NER%20Tagger/ner_dataset.csv.gz',\n",
        "                 compression='gzip', encoding='ISO-8859-1')\n",
        "\n",
        "# Use forward fill to replace any missing values in the DataFrame with the last known value.\n",
        "# This is particularly useful for filling in consecutive rows that are part of the same sentence\n",
        "# but have missing data due to the way the dataset is structured.\n",
        "df = df.fillna(method='ffill')\n",
        "\n",
        "# Display information about the DataFrame, such as the number of entries, columns, and data types.\n",
        "# This helps in understanding the structure of the dataset and identifying potential issues.\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of4cPvmBFgEr"
      },
      "outputs": [],
      "source": [
        "# Display the DataFrame to show an example of the dataset.\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7ZASLP3FqDr"
      },
      "outputs": [],
      "source": [
        "# Display the count of unique sentences, words, POS tags, and tags in the dataset\n",
        "unique_sentences = df['Sentence #'].nunique()\n",
        "unique_words = df['Word'].nunique()\n",
        "unique_pos_tags = df['POS'].nunique()\n",
        "unique_tags = df['Tag'].nunique()\n",
        "unique_sentences, unique_words, unique_pos_tags, unique_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Abtq8hi_G5yB"
      },
      "outputs": [],
      "source": [
        "# Display the frequency of each tag in the dataset\n",
        "tag_counts = df['Tag'].value_counts()\n",
        "tag_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L89tZVkFHK6X"
      },
      "source": [
        "**Tag Explanation (Recap)** <br>\n",
        "`IOB tagging Format:`\n",
        "*   I- prefix before a tag indicates that the tag is inside a chunk.\n",
        "*   B- prefix before a tag indicates that the tag is the beginning of a chunk.\n",
        "*   O- tag indicates that a token belongs to no chunk (outside).\n",
        "\n",
        "`The tags in this dataset:`\n",
        "\n",
        "*  geo = Geographical Entity\n",
        "*  org = Organization\n",
        "*  per = Person\n",
        "*  gpe = Geopolitical Entity\n",
        "*  tim = Time indicator\n",
        "*  art = Artifact\n",
        "*  eve = Event\n",
        "*  nat = Natural Phenomenon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw73jdgTHG-K"
      },
      "outputs": [],
      "source": [
        "# Define a function that extracts features from a word within a sentence\n",
        "def word2features(sent, i):\n",
        "    # Extract the word and part-of-speech (POS) tag at the given index 'i'\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "\n",
        "    #################### ADD YOUR CODE HERE################################\n",
        "    # Initialize a dictionary to store features for the current word\n",
        "    features = {\n",
        "        'bias': 1.0,  # A constant feature to help with learning algorithms\n",
        "        'word.lower()': word.lower(),  # The word in lowercase\n",
        "        ####YOUR CODE#####,  # Last three characters of the word (for suffix-based features)\n",
        "        ####YOUR CODE#####,  # Last two characters of the word\n",
        "        ####YOUR CODE#####,  # Check if the word is in uppercase\n",
        "        ####YOUR CODE#####,  # Check if the word is title-cased (first letter capitalized)\n",
        "        ####YOUR CODE#####,  # Check if the word is numeric\n",
        "        ####YOUR CODE#####,  # The POS tag of the word\n",
        "        ####YOUR CODE#####  # The first two characters of the POS tag (for coarse POS categories)\n",
        "    }\n",
        "\n",
        "    # If there is a previous word in the sentence, add features for the previous word\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            ####YOUR CODE#####,  # Lowercase form of the previous word\n",
        "            ####YOUR CODE#####,  # Check if the previous word is title-cased\n",
        "            ####YOUR CODE#####,  # Check if the previous word is in uppercase\n",
        "            ####YOUR CODE#####,  # POS tag of the previous word\n",
        "            ####YOUR CODE#####,  # First two characters of the previous POS tag\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True  # Mark the beginning of the sentence\n",
        "\n",
        "    # If there is a next word in the sentence, add features for the next word\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            ####YOUR CODE#####,  # Lowercase form of the next word\n",
        "            ####YOUR CODE#####,  # Check if the next word is title-cased\n",
        "            ####YOUR CODE#####,  # Check if the next word is in uppercase\n",
        "            ####YOUR CODE#####,  # POS tag of the next word\n",
        "            ####YOUR CODE#####,  # First two characters of the next POS tag\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True  # Mark the end of the sentence\n",
        "\n",
        "    # Return the feature dictionary for the current word\n",
        "    return features\n",
        "\n",
        "# Function to convert a sentence into a list of feature dictionaries for each word\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "# Function to extract labels from a sentence\n",
        "# Assumes each element in the sentence is a tuple (word, pos, label)\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7_5ZaoZIt_C"
      },
      "outputs": [],
      "source": [
        "# Define an aggregation function that combines 'Word', 'POS', and 'Tag' columns into a list of tuples\n",
        "# Each tuple represents a word with its corresponding POS tag and Tag\n",
        "agg_func = lambda s: [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
        "                                                   s['POS'].values.tolist(),\n",
        "                                                   s['Tag'].values.tolist())]\n",
        "\n",
        "# Group the DataFrame by 'Sentence #' and apply the aggregation function to each group\n",
        "# This creates a DataFrame where each sentence is represented as a list of (Word, POS, Tag) tuples\n",
        "grouped_df = df.groupby('Sentence #').apply(agg_func)\n",
        "\n",
        "# Convert the grouped sentences into a list of sentences, where each sentence is a list of tuples\n",
        "sentences = [s for s in grouped_df]\n",
        "\n",
        "# Display the first sentence as an example, showing it as a list of (Word, POS, Tag) tuples\n",
        "sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNatMB6XI0oD",
        "outputId": "21cb9a8e-a0b8-4a2b-d246-44dab892ddba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'bias': 1.0,\n",
              "  'word.lower()': 'through',\n",
              "  'word[-3:]': 'ugh',\n",
              "  'word[-2:]': 'gh',\n",
              "  'word.isupper()': False,\n",
              "  'word.istitle()': False,\n",
              "  'word.isdigit()': False,\n",
              "  'postag': 'IN',\n",
              "  'postag[:2]': 'IN',\n",
              "  'BOS': True,\n",
              "  '+1:word.lower()': 'london',\n",
              "  '+1:word.istitle()': True,\n",
              "  '+1:word.isupper()': False,\n",
              "  '+1:postag': 'NNP',\n",
              "  '+1:postag[:2]': 'NN'},\n",
              " {'bias': 1.0,\n",
              "  'word.lower()': 'london',\n",
              "  'word[-3:]': 'don',\n",
              "  'word[-2:]': 'on',\n",
              "  'word.isupper()': False,\n",
              "  'word.istitle()': True,\n",
              "  'word.isdigit()': False,\n",
              "  'postag': 'NNP',\n",
              "  'postag[:2]': 'NN',\n",
              "  '-1:word.lower()': 'through',\n",
              "  '-1:word.istitle()': False,\n",
              "  '-1:word.isupper()': False,\n",
              "  '-1:postag': 'IN',\n",
              "  '-1:postag[:2]': 'IN',\n",
              "  'EOS': True}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply the feature extraction function to a slice of a sample sentence\n",
        "# Here, it extracts features for the words at positions 5 and 6 in the first sentence\n",
        "sent2features(sentences[0][5:7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcKtGKSOLUYP"
      },
      "outputs": [],
      "source": [
        "# Example usage: extract labels for a subset of words from the first sentence\n",
        "sent2labels(sentences[0][0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwOdqXTrOMf9"
      },
      "outputs": [],
      "source": [
        "# Convert the list of sentences into lists of feature sets\n",
        "# Each sentence is transformed into a list of feature dictionaries using sent2features\n",
        "X = [sent2features(s) for s in sentences]\n",
        "\n",
        "# Convert the list of sentences into lists of label sets\n",
        "# Each sentence is transformed into a list of labels using sent2labels\n",
        "y = [sent2labels(s) for s in sentences]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# 25% of the data will be used for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Display the lengths of the training and testing sets to confirm the split\n",
        "len(X_train), len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2n6HtgZMkEm"
      },
      "outputs": [],
      "source": [
        "# Initialize a CRF model with specific hyperparameters:\n",
        "# - 'lbfgs': Uses the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) optimization algorithm,\n",
        "#   which is commonly used for training large-scale machine learning models.\n",
        "# - 'c1' and 'c2': Regularization parameters that control L1 and L2 regularization, respectively,\n",
        "#   helping to prevent overfitting. Here, both are set to 0.1.\n",
        "# - 'max_iterations': The maximum number of training iterations to run. Here, it is set to 100.\n",
        "# - 'all_possible_transitions': When True, considers all possible state transitions, which can improve accuracy.\n",
        "# - 'verbose': Enables verbose output to monitor the training progress and optimization details.\n",
        "crf = sklearn_crfsuite.CRF(algorithm='lbfgs',\n",
        "                           c1=0.1,\n",
        "                           c2=0.1,\n",
        "                           max_iterations=100,\n",
        "                           all_possible_transitions=True,\n",
        "                           verbose=True)\n",
        "\n",
        "# Train the CRF model on the training data.\n",
        "# The fit method takes the feature sets (X_train) and corresponding labels (y_train) as inputs.\n",
        "\n",
        "##### ADD YOUR CODE HERE #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJGnDHELM592"
      },
      "outputs": [],
      "source": [
        "# Use the trained CRF model to predict labels for the test data (X_test).\n",
        "# The predict method generates a list of predicted label sequences for each sentence in X_test.\n",
        "# Each element in y_pred is a list of predicted labels corresponding to one sentence.\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "# Print the predicted labels for the first test sentence.\n",
        "# This provides a quick look at the model's output, showing how it has labeled each word in the sentence.\n",
        "print(y_pred[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzAX22XINK_K"
      },
      "outputs": [],
      "source": [
        "# Print the list of true labels for the first sentence in the test set.\n",
        "# Each element in the list corresponds to the named entity label for a word in the sentence.\n",
        "print(y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LaJhnVPNMYC"
      },
      "outputs": [],
      "source": [
        "# Get a list of all labels (classes) that the CRF model has learned.\n",
        "# This includes the named entity labels and possibly the 'O' label for non-entities.\n",
        "labels = list(crf.classes_)\n",
        "\n",
        "# Remove the 'O' label from the list of labels, as it's often excluded from evaluation.\n",
        "# The 'O' label represents non-entity words, and excluding it focuses the evaluation on entity recognition performance.\n",
        "labels.remove('O')\n",
        "\n",
        "# Print a detailed classification report showing precision, recall, and F1-score for each label.\n",
        "# The flat_classification_report function computes metrics for each entity type across all words in all sentences.\n",
        "# It \"flattens\" the lists of labels (y_test and y_pred) into single lists for easier computation.\n",
        "print(crf_metrics.flat_classification_report(y_test, y_pred, labels=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRASFVuOXRm5"
      },
      "source": [
        "# **Build an End-to-End NER Tagger with a Trained NER Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQeZy9aJWsF2"
      },
      "outputs": [],
      "source": [
        "# example text\n",
        "text = \"\"\"Last week, Google announced a partnership with Microsoft to develop a new cloud computing platform. The project, named Project Quantum, aims to integrate artificial intelligence and machine learning into cloud services. According to Sundar Pichai, Google’s CEO, this platform will leverage the capabilities of Microsoft Azure and Google Cloud to provide enhanced data analytics and storage solutions. During a press conference in San Francisco, Satya Nadella, the CEO of Microsoft, mentioned that this collaboration could significantly reduce operational costs for enterprises. The initiative is expected to launch by early 2025, with beta testing available by mid-2024. Additionally, Amazon Web Services (AWS) was mentioned as a key competitor in this space, as they recently released updates to their own AI-powered services, including AWS Lambda and SageMaker. In a related development, cybersecurity firms such as Palo Alto Networks and CrowdStrike have started exploring partnerships with cloud providers to enhance security measures. This trend reflects the growing need for comprehensive solutions that can protect sensitive data from cyber threats, especially with the rise of remote work and increasing dependence on virtual private networks (VPNs).\"\"\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbtTRP8AXNXa"
      },
      "outputs": [],
      "source": [
        "# This splits the text into individual word tokens, which are necessary for POS tagging and feature extraction.\n",
        "# Apply Part-of-Speech (POS) tagging to the list of word tokens.\n",
        "# The nltk.pos_tag function returns a list of tuples, where each tuple contains a word and its POS tag.\n",
        "# Extract features for each word in the tokenized and POS-tagged text.\n",
        "# The sent2features function transforms the list of (word, POS) tuples into feature dictionaries for each word.\n",
        "##### ADD YOUR CODE HERE #####\n",
        "text_tokens = #####CODE HERE #####\n",
        "text_pos = #####CODE HERE #####\n",
        "features = #####CODE HERE #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1YwT-mOYfLx"
      },
      "outputs": [],
      "source": [
        "# Use the trained CRF model to predict NER tags for the input features.\n",
        "labels = #####CODE HERE #####\n",
        "\n",
        "# Extract the predicted labels for the entire document (first and only item in 'labels').\n",
        "# Each label corresponds to a named entity tag for each token in the input text.\n",
        "doc_labels = labels[0]\n",
        "\n",
        "# Pair each token with its predicted named entity tag.\n",
        "# 'text_ner' is a list of tuples, where each tuple contains a word from the text and its predicted tag.\n",
        "text_ner = [(token, tag) for token, tag in zip(text_tokens, doc_labels)]\n",
        "\n",
        "# Initialize an empty list to collect named entities and variables to temporarily store entity names and tags.\n",
        "named_entities = []\n",
        "temp_entity_name = ''\n",
        "temp_named_entity = None\n",
        "\n",
        "# Iterate over each token and its tag to extract named entities.\n",
        "for term, tag in text_ner:\n",
        "    # If the tag is not 'O' (outside), the token is part of a named entity.\n",
        "    if tag != 'O':\n",
        "        # Append the current term to the entity name (useful for multi-word entities).\n",
        "        temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
        "        # Update the temporary named entity with the current entity name and its tag.\n",
        "        temp_named_entity = (temp_entity_name, tag)\n",
        "    else:\n",
        "        # If an 'O' tag is encountered and there is a stored entity, save the named entity.\n",
        "        if temp_named_entity:\n",
        "            named_entities.append(temp_named_entity)\n",
        "            # Reset temporary variables for the next potential entity.\n",
        "            temp_entity_name = ''\n",
        "            temp_named_entity = None\n",
        "\n",
        "# Convert the list of named entities to a DataFrame for easier visualization.\n",
        "# Each named entity is a row with 'Entity' and 'Tag' columns, showing the entity's text and its predicted tag.\n",
        "pd.DataFrame(named_entities, columns=['Entity', 'Tag']).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP1sOEc7YkH6"
      },
      "outputs": [],
      "source": [
        "# compare the result with spacy library\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text_nlp = nlp(text)\n",
        "displacy.render(text_nlp, style='ent', jupyter=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
