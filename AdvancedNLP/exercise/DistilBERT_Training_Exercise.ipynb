{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5RbCOIM9I82"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Emgg6nS9SKJ"
      },
      "outputs": [],
      "source": [
        "# Load the IMDB movie reviews dataset from the Hugging Face dataset library\n",
        "imdb = load_dataset(\"imdb\")\n",
        "imdb[\"test\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bMzL2Kn9UuU"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained tokenizer (DistilBERT model, uncased) from the Hugging Face library\n",
        "tokenizer = ##### YOUR CODE #####\n",
        "\n",
        "# Define a preprocessing function that tokenizes the input text and applies truncation\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "# Apply the preprocessing function to the IMDB dataset using the 'map' method,\n",
        "# which processes the dataset in batches and returns the tokenized version\n",
        "tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "\n",
        "# Create a data collator that will dynamically pad the input sequences to the same length\n",
        "# using the tokenizer. This ensures that batches of tokenized text are uniformly padded for training or evaluation.\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Create a dictionary to map class indices to human-readable labels for sentiment analysis\n",
        "# 0 represents \"NEGATIVE\" sentiment, and 1 represents \"POSITIVE\" sentiment\n",
        "# Create a reverse dictionary to map sentiment labels back to their respective class indices\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQeptLYa9bNR"
      },
      "outputs": [],
      "source": [
        "# Load the 'accuracy' metric from the Hugging Face evaluation library\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Define a function to compute evaluation metrics\n",
        "# The function takes the evaluation predictions and ground truth labels as input\n",
        "def compute_metrics(eval_pred):\n",
        "    # Unpack the predictions and labels from the evaluation tuple\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Convert the model's raw output (logits) into predicted class labels using argmax\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Compute the accuracy by comparing predictions with reference labels\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA4nVlXi9dmm"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained DistilBERT model for sequence classification from the Hugging Face library\n",
        "# The model is initialized for a binary classification task (num_labels=2)\n",
        "# The id2label and label2id mappings are provided to associate class indices with their respective sentiment labels\n",
        "model = ##### YOUR CODE #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np9MCmuA9kd-"
      },
      "outputs": [],
      "source": [
        "# Define the training arguments for the Trainer API\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",            # Directory where the model checkpoints will be saved\n",
        "    learning_rate=2e-5,                       # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=16,           # Batch size per device for training\n",
        "    per_device_eval_batch_size=16,            # Batch size per device for evaluation\n",
        "    num_train_epochs=2,                       # Number of training epochs\n",
        "    weight_decay=0.01,                        # Weight decay for regularization\n",
        "    evaluation_strategy=\"epoch\",              # Evaluate the model at the end of each epoch\n",
        "    save_strategy=\"epoch\",                    # Save the model at the end of each epoch\n",
        "    load_best_model_at_end=True,              # Load the best model based on evaluation at the end of training\n",
        "    push_to_hub=False,                        # Whether to push the model to the Hugging Face Model Hub (disabled)\n",
        ")\n",
        "\n",
        "# Initialize the Trainer class with the model, training arguments, datasets, and other components\n",
        "trainer = Trainer(\n",
        "    model=model,                              # Pre-trained DistilBERT model for sequence classification\n",
        "    args=training_args,                       # Training arguments specified above\n",
        "    train_dataset=tokenized_imdb[\"train\"],    # Tokenized IMDB training dataset\n",
        "    eval_dataset=tokenized_imdb[\"test\"],      # Tokenized IMDB test dataset for evaluation\n",
        "    tokenizer=tokenizer,                      # Tokenizer for processing input text\n",
        "    data_collator=data_collator,              # Data collator for padding sequences in batches\n",
        "    compute_metrics=compute_metrics,          # Function to compute evaluation metrics (accuracy)\n",
        ")\n",
        "\n",
        "# Start training the model using the specified trainer and training arguments\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
